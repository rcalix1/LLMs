## Fine Tune GPTs and warm starting Transformers

On fine tuning  GPTs and warm starting Transformers

## Code Bases

* Alpaca (https://github.com/tatsu-lab/stanford_alpaca)
* Llama GitHub (https://github.com/facebookresearch/llama)
* Dolly (https://github.com/databrickslabs/dolly)
* GPT4all (https://github.com/nomic-ai/gpt4all)
* Warm-Starting code (https://huggingface.co/blog/warm-starting-encoder-decoder)

## Readings:

* Llama (https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
* Llama (https://arxiv.org/abs/2302.13971v1)
* Warm starting paper: Sascha Rothe, Shashi Narayan, Aliaksei Severyn. (2019). Leveraging Pre-trained Checkpoints for Sequence Generation Tasks. (https://arxiv.org/pdf/1907.12461.pdf)
* Self-Instruct (https://arxiv.org/pdf/2212.10560.pdf)
* Hyung Won Chung et al. (2022). Scaling Instruction-Finetuned Language Models. https://arxiv.org/pdf/2210.11416.pdf
